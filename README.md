# Random-Forest-Regression
This project includes C++ code for decision tree regression (decisiontree.cpp) and random forest regression (randomforest.cpp) algorithms. Both algorithms use bottom-up cost complexity pruning as well as K-fold cross-validation to determine the best cost complexity coefficient. I decided against a simpler algorithm that terminates the decision tree after some threshold reduction in MSE because one data split may contribute very little to the overall information gain, while subsequent splits are important. This goes overlooked without creating a deep tree. Instead, the data is overfitted, then pruned back according to the methods mentioned above. The data is imported through text files (by fstream and sstream) and undergoes a random ~80/20 training/test data split after it is properly initialized into a vector<vector<double>> structure of the form {independent, target}. 

The algorithms use two structures, Node* and Split*. The Node* structure contains the split value that has the greatest information gain (or greatest reduction MSE), the "feature index" of the split (the best independent variable to apply a data split to), the training data set that satisfies the conditions of the node, as well as pointers to the parent node, the left child node, and the right child node. If the node is the root node, the parent node will be set to nullptr, and if the node is a leaf node, the child nodes are set to nullptr. The decision tree is built with a queue data structure (queue<Node*>). The decisionTree function in decisiontree.cpp and the randomForest function in randomforest.cpp loop through each element in the queue, either creating child nodes and adding them to the queue, or setting the given node to a leaf node if the terminating conditions are met. The front element of the queue is removed for every loop.

The random forest is a simple modification of the decision tree code. Rather than consider the full training data of size $M$ set at each split, a random subset of size $N = \sqrt{M}$ is considered. This process reduces the correlation between variables and speeds up the algorithm (by selecting smaller subsets at each split) at very little cost to the fit (for AAPL price data, R-Squared changes by <0.01). There is another random forest method that is not implemented here, which is to randomly sample the training data with replacement to obtain another vector of size $M$, then form a decision tree with this alternative data set. This process is repeated a given number of times and the predicted values within each split interval are averaged.
